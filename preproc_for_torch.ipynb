{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #           Word  POS Tag\n",
       "0         1.0      Thousands  NNS   O\n",
       "1         1.0             of   IN   O\n",
       "2         1.0  demonstrators  NNS   O\n",
       "3         1.0           have  VBP   O\n",
       "4         1.0        marched  VBN   O"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('clean_data/clean_data.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thousands\n",
      "of\n",
      "demonstrators\n",
      "have\n",
      "marched\n",
      "through\n",
      "London\n",
      "to\n",
      "protest\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "for r in df.values[:10]:\n",
    "    print(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll first turn our dataframe to a list of lists of tuples, since this is the infinitely\n",
    "# more convenient data structure for neural nets and torch\n",
    "\n",
    "df.Word.to_list()\n",
    "\n",
    "def df_to_torch_list(df):\n",
    "    \"\"\"Function takes in dataframe with four columns:\n",
    "    Sentence #; Word; POS; Tag.\n",
    "    -------------------------------------------------\n",
    "    Returns: \n",
    "    - input_data as a list of lists (each a sentence) of tuples\n",
    "    where each tuple is (word; POS)\n",
    "    - target_data - list of lists (each a sentence) of Named \n",
    "    Entity Tags (e.g. 'O', 'B-geo', 'I-art', etc)\n",
    "    \"\"\"\n",
    "    \n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    data = df.copy()\n",
    "    for sent_ind in range(1,len(data['Sentence #'].unique().astype(int))):\n",
    "        sent_df = data.loc[data['Sentence #'] == sent_ind]\n",
    "        sent_lst = []\n",
    "        sent_target_lst = []\n",
    "        for row in sent_df.values:\n",
    "            sent_lst.append((row[1], row[2]))\n",
    "            sent_target_lst.append(row[3])\n",
    "        input_data.append(sent_lst)\n",
    "        target_data.append(sent_target_lst)\n",
    "    return input_data, target_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "input_data, target_data = df_to_torch_list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Families', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('soldiers', 'NNS'),\n",
       " ('killed', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('conflict', 'NN'),\n",
       " ('joined', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('protesters', 'NNS'),\n",
       " ('who', 'WP'),\n",
       " ('carried', 'VBD'),\n",
       " ('banners', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('such', 'JJ'),\n",
       " ('slogans', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('\"', '.'),\n",
       " ('Bush', 'NNP'),\n",
       " ('Number', 'NN'),\n",
       " ('One', 'CD'),\n",
       " ('Terrorist', 'NN'),\n",
       " ('\"', '.'),\n",
       " ('and', 'CC'),\n",
       " ('\"', '.'),\n",
       " ('Stop', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('Bombings', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('\"', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[1]\n",
    "# target_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data[2548])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know what lenght of our longest sentence is. This is important since our neural net will require all inputs to be of equal length and we'll pad shorter sentences to length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "\n",
    "Before any preprocessing occurs we will split our data into training, validation and test datasets. This might seem strange to do before creating the vocabulary, but this way, when we do get to validation and testing stage, we'll be able to see just how much 'unknown' words will impact on the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, test_sents, train_labels, test_labels = train_test_split(input_data, target_data, test_size=.2, shuffle=False)\n",
    "train_sents, valid_sents, train_labels, valid_labels = train_test_split(train_sents, train_labels, test_size=.15, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing into OHE\n",
    "Workflow for data preprocessing for pytorch (based on the following [resource posted by Andrew Ng and Stanford University's CS Department](https://cs230.stanford.edu/blog/namedentity/)). \n",
    "\n",
    "1. Create a unique vocabulary dict\n",
    "2. Create an NE tags dict\n",
    "3. Turning text data into lists of ints\n",
    "4. Using a batch generator to turn lists into Torch Tensors\n",
    "5. Specifying a lookup table for turning tensors to embedded arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thousands': 1,\n",
       " 'of': 106,\n",
       " 'demonstrators': 3,\n",
       " 'have': 4,\n",
       " 'marched': 56,\n",
       " 'through': 6,\n",
       " 'London': 7,\n",
       " 'to': 62,\n",
       " 'protest': 85,\n",
       " 'the': 101,\n",
       " 'war': 11,\n",
       " 'in': 100,\n",
       " 'Iraq': 13,\n",
       " 'and': 48,\n",
       " 'demand': 15,\n",
       " 'withdrawal': 17,\n",
       " 'British': 19,\n",
       " 'troops': 20,\n",
       " 'from': 57,\n",
       " 'that': 22,\n",
       " 'country': 23,\n",
       " '.': 108,\n",
       " 'UNK': 110,\n",
       " 'PAD': 0,\n",
       " 'Families': 25,\n",
       " 'soldiers': 27,\n",
       " 'killed': 28,\n",
       " 'conflict': 31,\n",
       " 'joined': 32,\n",
       " 'protesters': 34,\n",
       " 'who': 35,\n",
       " 'carried': 36,\n",
       " 'banners': 37,\n",
       " 'with': 38,\n",
       " 'such': 39,\n",
       " 'slogans': 40,\n",
       " 'as': 41,\n",
       " '\"': 54,\n",
       " 'Bush': 43,\n",
       " 'Number': 44,\n",
       " 'One': 45,\n",
       " 'Terrorist': 46,\n",
       " 'Stop': 50,\n",
       " 'Bombings': 52,\n",
       " 'They': 55,\n",
       " 'Houses': 59,\n",
       " 'Parliament': 61,\n",
       " 'a': 63,\n",
       " 'rally': 64,\n",
       " 'Hyde': 66,\n",
       " 'Park': 67,\n",
       " 'Police': 69,\n",
       " 'put': 70,\n",
       " 'number': 72,\n",
       " 'marchers': 74,\n",
       " 'at': 75,\n",
       " '10,000': 76,\n",
       " 'while': 77,\n",
       " 'organizers': 78,\n",
       " 'claimed': 79,\n",
       " 'it': 80,\n",
       " 'was': 81,\n",
       " '1,00,000': 82,\n",
       " 'The': 84,\n",
       " 'comes': 86,\n",
       " 'on': 87,\n",
       " 'eve': 89,\n",
       " 'annual': 92,\n",
       " 'conference': 93,\n",
       " 'Britain': 95,\n",
       " \"'s\": 96,\n",
       " 'ruling': 97,\n",
       " 'Labor': 98,\n",
       " 'Party': 99,\n",
       " 'southern': 102,\n",
       " 'English': 103,\n",
       " 'seaside': 104,\n",
       " 'resort': 105,\n",
       " 'Brighton': 107}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_int_vocab(input_data : list):\n",
    "    \"\"\"Function takes in list (corpus) of lists (sentences) of dicts (words) of tuples (word, POS) and returns\n",
    "    a single vocabulary dict.\n",
    "    Returns:\n",
    "    vocab_dict - (dict) word - unique integer pairs.\"\"\"\n",
    "    vocab_dict = {}\n",
    "    i=1\n",
    "    for sentence in input_data:\n",
    "        for word_pos_tuple in sentence:\n",
    "            if word_pos_tuple not in vocab_dict.keys():\n",
    "                vocab_dict[word_pos_tuple[0]] = i\n",
    "                i +=1\n",
    "                continue\n",
    "            else: \n",
    "                continue\n",
    "        vocab_dict['UNK'] = i+1\n",
    "        vocab_dict['PAD'] = 0\n",
    "    return vocab_dict\n",
    "\n",
    "vocab_test = generate_int_vocab(train_sents[:5])\n",
    "vocab_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 18.8 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7117"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "vocabulary_dict = generate_int_vocab(train_sents)\n",
    "len(vocabulary_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having previously run the vocab generator on the entire dataset, the number of unique terms was 8766. Now reduced to 7115 we can gauge that at least 1.5k words will be be assigned the unknown tag across the validation and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "def generate_tag_dict(targets_list):\n",
    "    \"\"\"Function takes in a list of NE tags (which should include at least one instance of \n",
    "    every possible NE tag) and returns a dict matching each NE tag to a unique int.\n",
    "    Returns:\n",
    "    tag_map - (dict) of NE tag - associated int pairs\"\"\"\n",
    "    ne_dict = {}\n",
    "    i = 0\n",
    "    for sublist in targets_list:\n",
    "        for ne in sublist:\n",
    "            if ne in ne_dict.keys():\n",
    "                continue\n",
    "            else:\n",
    "                ne_dict[ne] = i\n",
    "                i += 1\n",
    "    return ne_dict\n",
    "\n",
    "ne_dict = generate_tag_set(train_labels)\n",
    "len(ne_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_ints(feature_list : list, targets_list : list, vocab : dict, ne_dict : dict, incl_POS = False, POS_dict = None):\n",
    "    \"\"\"Function takes in list (corpus) of lists (sentences) of dicts (words) of tuples (word, POS) and returns\n",
    "    a list (corpus) of lists (sentences) of integers (representing words).\n",
    "    Returns:\n",
    "    list_data.\"\"\"\n",
    "    int_sentences = []        \n",
    "    int_label_sentences = []\n",
    "    \n",
    "    for sentence in feature_list:     \n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "        sentence_ints = [vocab[token[0].lower()] if token[0].lower() in vocab.keys() \n",
    "             else vocab['UNK']\n",
    "             for token in sentence]\n",
    "        int_sentences.append(sentence_ints)\n",
    "        \n",
    "    for sentence in targets_list:\n",
    "        #replace each label by its index\n",
    "        label_sent = [ne_dict[label] for label in sentence]\n",
    "        int_label_sentences.append(label_sent) \n",
    "        \n",
    "        \n",
    "    if incl_POS:\n",
    "        int_sentences_POS = []\n",
    "        for sentence in feature_list:     \n",
    "        #replace each token by its index if it is in vocab\n",
    "        #else use index of UNK\n",
    "            sentence_POS_int = [vocab[token[1]] if token[1] in POS_dict.keys() \n",
    "                 else POS_dict['UNK_POS']\n",
    "                 for token in sentence]\n",
    "            int_sentences_POS.append(sentence_POS_int)\n",
    "        return int_sentences, int_sentences_POS, int_label_sentences\n",
    "        \n",
    "    else:     \n",
    "        return int_sentences, int_label_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 10 µs, total: 14 µs\n",
      "Wall time: 16 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# for now, we will only be generating the feature data w/out the POS tags\n",
    "train_int_sentences, train_int_label_sentences = sent_to_ints(train_sents, train_labels, vocabulary_dict, ne_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, a quick visual inspection to make sure everything has worked according to plan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands', 'NNS'), ('of', 'IN'), ('demonstrators', 'NNS'), ('have', 'VBP')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44346\n",
      "45240\n",
      "44310\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_dict['Thousands'])\n",
    "print(vocabulary_dict['of'])\n",
    "print(vocabulary_dict['demonstrators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43858, 45240, 44310, 45232]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_int_sentences[0][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generation\n",
    "We'll be defining a new object class of batch generators which will section our data and turn several sentences at a time to Torch Tensor objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing_for_torch as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    \n",
    "    def __init__(self, vocab, ne_dict):\n",
    "        self.vocab = vocab\n",
    "        self.ne_dict = ne_dict\n",
    "        \n",
    "\n",
    "    def prep_batch(self, batch_sentences : list, batch_sentences_labels : list, word_vect_dim = 50):\n",
    "        \"\"\"Function takes in a list of lists (each sublist a sentence of n-dimension\n",
    "        numpy arrays), the associated list of lists of NE labels and a vocabulary (dict)\"\"\"\n",
    "        vocab = self.vocab\n",
    "        \n",
    "        #compute length of longest sentence in batch\n",
    "        batch_max_len = max([len(sentence) for sentence in batch_sentences_labels])\n",
    "        self.batch_max_len = batch_max_len\n",
    "        #prepare a numpy array with the data, initializing the data with 'PAD' \n",
    "        #and all labels with -1; initializing labels to -1 differentiates tokens \n",
    "        #with tags from 'PAD' tokens\n",
    "        #note the dimensional change here as we are effectively about to \n",
    "        # concatenate the sentences along the 2nd dimension\n",
    "        batch_data = vocab['PAD']*np.ones((len(batch_sentences), batch_max_len))\n",
    "        batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "        \n",
    "        #copy the data to the numpy array\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "            batch_labels[j][:cur_len] = batch_sentences_labels[j]\n",
    "\n",
    "        #since all data are indices, we convert them to torch LongTensors\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "\n",
    "        #convert Tensors to Variables\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "        yield batch_data, batch_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = DataGenerator(vocabulary_dict, ne_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, a quick visual inspection will show us that our sentences and associated labels have been transformed into equal length tensors, where 0 refers to padding and -1 is the corresponding output tag for the padding. This labelling is important since the model needs to know to distinguish non-NE words (tag : 0) from the artificial padding (tag : -1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[43858, 45240, 44310, 45232, 44349, 44350, 45245, 45139, 19267, 45238,\n",
       "          44827, 45216, 45245, 45223, 41355, 45238, 44902, 45240, 45245, 40617,\n",
       "          44835, 45161, 45199, 45243,     0,     0,     0,     0,     0,     0],\n",
       "         [38307, 45240, 43847, 44672, 45216, 45238, 42784, 41145, 45238, 44418,\n",
       "          44657, 44851,    37, 44986, 44910, 33097, 45036, 45204, 45245, 44215,\n",
       "          43885, 40415, 45204, 45223, 45204, 41858, 45238, 38892, 45243, 45204],\n",
       "         [45025, 44349, 44835, 45238, 12048, 45240, 44867, 45139, 45184, 43944,\n",
       "          45216, 45245, 38317, 45243,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
       "           2,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0, -1, -1, -1, -1,\n",
       "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_gen.prep_batch(train_int_sentences[0:3], train_int_label_sentences[0:3],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "data_batch_generator = data_gen.prep_batch(train_int_sentences[:10], train_int_label_sentences[:10],)\n",
    "train_batch_1_sent, train_batch_1_labels = next(data_batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
       "          2,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  4,  0, -1, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  5,  6,  0,  0,\n",
       "          0,  2,  0,  0,  0,  1,  0, -1, -1, -1, -1, -1],\n",
       "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "          2,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1],\n",
       "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,\n",
       "          0,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "        [ 0,  5,  6,  6,  6,  0,  0,  0,  0,  0,  0,  0,  0,  1,  7,  0,  0,  0,\n",
       "          0,  0,  2,  0,  0,  0,  0,  0,  0,  0, -1, -1],\n",
       "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0, -1, -1, -1,\n",
       "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "        [ 2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,\n",
       "          0,  5,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_1_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NER_Net(nn.Module):\n",
    "    def __init__(self, vocab, embedding_dim : int, lstm_hidden_dim : int, ne_tags : dict):\n",
    "        # inherits attributes of the super class `Net` \n",
    "        super(NER_Net, self).__init__()\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.ne_tags = ne_tags\n",
    "        self.number_of_tags = len(ne_tags)\n",
    "        \n",
    "        \n",
    "        #maps each token to an embedding_dim vector\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        #the LSTM takens embedded sentence\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        #fc layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(self.lstm_hidden_dim, self.number_of_tags)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, s):\n",
    "        #apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim\n",
    "\n",
    "        #run the LSTM along the sentences of length batch_max_len\n",
    "        s, _ = self.lstm(s)     # dim: batch_size x batch_max_len x lstm_hidden_dim                \n",
    "\n",
    "        #reshape the Variable so that each row contains one token\n",
    "        s = s.view(-1, s.shape[2])  # dim: batch_size*batch_max_len x lstm_hidden_dim\n",
    "\n",
    "        #apply the fully connected layer and obtain the output for each token\n",
    "        s = self.fc(s)          # dim: batch_size*batch_max_len x num_tags\n",
    "\n",
    "        return F.log_softmax(s, dim=1)   # dim: batch_size*batch_max_len x num_tags\n",
    "    \n",
    "    def loss_fn(self, outputs, labels):\n",
    "        #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "        labels = labels.view(-1)  \n",
    "\n",
    "        #mask out 'PAD' tokens\n",
    "        mask = (labels >= 0).float()\n",
    "\n",
    "        #the number of tokens is the sum of elements in mask\n",
    "        num_tokens = int(torch.sum(mask).data[0])\n",
    "\n",
    "        #pick the values corresponding to labels and multiply by mask\n",
    "        outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
    "\n",
    "        #cross entropy loss for all non 'PAD' tokens\n",
    "        return -torch.sum(outputs)/num_tokens\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = data_gen.prep_batch(train_int_sentences, train_int_label_sentences)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_classif = NER_Net(vocabulary_dict, 50, 50, ne_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-3ad94f083cd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#pass through model, perform backpropagation and updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner_classif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner_classif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-9526b32ff865>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#apply the embedding layer that maps each token to its embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# dim: batch_size x batch_max_len x embedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#run the LSTM along the sentences of length batch_max_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "\n",
    "num_training_steps=100\n",
    "\n",
    "for _ in range(num_training_steps):\n",
    "    batch_sentences, batch_labels = next(train_iterator)\n",
    "\n",
    "    #pass through model, perform backpropagation and updates\n",
    "    output_batch = ner_classif(batch_sentences)\n",
    "    \n",
    "    loss = ner_classif.loss_fn(output_batch, batch_labels)\n",
    "    \n",
    "\n",
    "    optimizer.zero_grad()  # clear previous gradients\n",
    "    loss.backward()        # compute gradients of all variables wrt loss\n",
    "\n",
    "    optimizer.step()    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding glove embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vects(file = 'glove/glove.6B.50d.txt', vdim=None):\n",
    "    \"\"\"Function that loads the Global representation Vectors\n",
    "    and returns them as a dictionary. \n",
    "    -----------------\n",
    "    Returns:\n",
    "    glove_dict - (dict) key - word (str), value - n-dimensional np array \"\"\"\n",
    "    glove_dict = {}\n",
    "#     total_vocab = vocab\n",
    "    if type(vdim)==int:\n",
    "        file = f'glove/glove.6B.{vdim}d.txt'\n",
    "    avg_vect = np.zeros((vdim,))\n",
    "    with open(file, 'rb') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0].decode('utf-8')\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove_dict[word] = vector\n",
    "            avg_vect += vector\n",
    "        # creating the vector for new, UNKnown words in the vocabulary\n",
    "        # NOTE, this is NOT the same as the word \"unk\", which is \n",
    "        # present in glove's vocabulary\n",
    "        glove_dict['UNK'] = avg_vect/len(glove_dict)\n",
    "        glove_dict['PAD'] = np.zeros((vdim,))\n",
    "    return glove_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 24.8 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "glove_dict = load_glove_vects(vdim=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12920061, -0.28866239, -0.01224894, -0.05676689, -0.20211109,\n",
       "       -0.08389026,  0.33359737,  0.16045146,  0.03867495,  0.17833092,\n",
       "        0.0469662 , -0.00285779,  0.29099851,  0.04613723, -0.20923842,\n",
       "       -0.066131  , -0.06822448,  0.07665885,  0.31339918,  0.17848512,\n",
       "       -0.12257719, -0.09916928, -0.07495973,  0.06413206,  0.14441256,\n",
       "        0.608946  ,  0.17463101,  0.05335403, -0.01273826,  0.03474108,\n",
       "       -0.81239567, -0.04688727,  0.20193533,  0.20311115, -0.03935654,\n",
       "        0.06967518, -0.01553655, -0.03405275, -0.06528025,  0.12250092,\n",
       "        0.13992005, -0.17446305, -0.08011841,  0.08495219, -0.01041645,\n",
       "       -0.13704901,  0.20127088,  0.10069294,  0.00653007,  0.0168515 ])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict['UNK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "* create input layer, matching incoming words to their respective glove_dict arrays\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vect_sent , train_label_sent = sent_to_vect(train_sents, train_labels, glove_dict, ne_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label_sent[200]) == len(train_vect_sent[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict['PAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1515e+00, -3.9703e-01,  9.7350e-01, -8.3455e-01, -1.4785e-01,\n",
       "       -4.7469e-01, -9.8629e-01,  4.4072e-01,  1.0985e-01,  7.3914e-03,\n",
       "       -4.5690e-01, -1.2794e+00,  1.0253e+00, -5.3370e-01,  1.0906e+00,\n",
       "       -3.6994e-01, -1.7323e-03, -1.2934e-02, -2.0921e-01, -8.0484e-01,\n",
       "        3.0218e-01,  2.9622e-01,  4.3949e-02, -6.2642e-02, -1.1756e-02,\n",
       "       -1.2806e+00, -2.3914e-01, -5.0524e-01,  2.8103e-01, -3.1305e-01,\n",
       "        3.0938e+00,  6.8201e-01, -3.8915e-01, -5.9624e-01, -6.8694e-01,\n",
       "        7.9195e-01, -1.5878e-01, -7.9453e-01, -2.0664e-01,  4.5275e-01,\n",
       "       -4.2613e-01,  3.5096e-01,  5.5050e-01,  2.5910e-01,  7.1832e-01,\n",
       "       -5.3633e-02, -1.0610e+00, -4.6405e-01, -9.2481e-01, -1.6236e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict['thousands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1515e+00, -3.9703e-01,  9.7350e-01, -8.3455e-01, -1.4785e-01,\n",
       "        -4.7469e-01, -9.8629e-01,  4.4072e-01,  1.0985e-01,  7.3914e-03,\n",
       "        -4.5690e-01, -1.2794e+00,  1.0253e+00, -5.3370e-01,  1.0906e+00,\n",
       "        -3.6994e-01, -1.7323e-03, -1.2934e-02, -2.0921e-01, -8.0484e-01,\n",
       "         3.0218e-01,  2.9622e-01,  4.3949e-02, -6.2642e-02, -1.1756e-02,\n",
       "        -1.2806e+00, -2.3914e-01, -5.0524e-01,  2.8103e-01, -3.1305e-01,\n",
       "         3.0938e+00,  6.8201e-01, -3.8915e-01, -5.9624e-01, -6.8694e-01,\n",
       "         7.9195e-01, -1.5878e-01, -7.9453e-01, -2.0664e-01,  4.5275e-01,\n",
       "        -4.2613e-01,  3.5096e-01,  5.5050e-01,  2.5910e-01,  7.1832e-01,\n",
       "        -5.3633e-02, -1.0610e+00, -4.6405e-01, -9.2481e-01, -1.6236e+00])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_1_sent[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0., -1., -1., -1., -1.,\n",
       "        -1., -1.])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_1_labels[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
