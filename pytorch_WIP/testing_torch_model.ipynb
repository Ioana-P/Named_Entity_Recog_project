{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NER_ETL import EntityETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = EntityETL('glove/glove.6B.50d.txt', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner.load_train_vocab_nn('clean_data/clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8768"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK_NE': 1,\n",
       " 'PAD': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-tim': 2,\n",
       " 'I-gpe': 3,\n",
       " 'B-gpe': 4,\n",
       " 'I-eve': 5,\n",
       " 'B-nat': 6,\n",
       " 'B-per': 7,\n",
       " 'B-eve': 8,\n",
       " 'I-tim': 9,\n",
       " 'I-per': 10,\n",
       " 'I-org': 11,\n",
       " 'I-nat': 12,\n",
       " 'I-geo': 13,\n",
       " 'B-org': 14,\n",
       " 'O': 15,\n",
       " 'B-art': 16,\n",
       " 'I-art': 17}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.ne_tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK_POS': 1,\n",
       " 'PAD': 0,\n",
       " 'PDT': 1,\n",
       " 'NNP': 2,\n",
       " 'POS': 3,\n",
       " 'NNPS': 4,\n",
       " 'NN': 5,\n",
       " 'VBP': 6,\n",
       " 'JJS': 7,\n",
       " 'WP': 8,\n",
       " 'VBZ': 9,\n",
       " 'WRB': 10,\n",
       " ':': 11,\n",
       " 'JJR': 12,\n",
       " 'RRB': 13,\n",
       " 'PRP': 14,\n",
       " 'RBS': 15,\n",
       " 'CC': 16,\n",
       " 'MD': 17,\n",
       " 'RB': 18,\n",
       " 'JJ': 19,\n",
       " 'VBG': 20,\n",
       " 'DT': 21,\n",
       " '$': 22,\n",
       " 'UH': 23,\n",
       " 'EX': 24,\n",
       " 'NNS': 25,\n",
       " 'VB': 26,\n",
       " 'CD': 27,\n",
       " 'VBN': 28,\n",
       " 'RBR': 29,\n",
       " 'PRP$': 30,\n",
       " 'RP': 31,\n",
       " 'TO': 32,\n",
       " 'WP$': 33,\n",
       " 'LRB': 34,\n",
       " 'VBD': 35,\n",
       " 'WDT': 36,\n",
       " '.': 37,\n",
       " 'IN': 38}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.pos_tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sent, train_labels = ner.load_train_input_data('clean_data/clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Other',\n",
       " 'options',\n",
       " 'include',\n",
       " 'moving',\n",
       " 'the',\n",
       " 'U.N.',\n",
       " 'mission',\n",
       " 'headquarters',\n",
       " 'from',\n",
       " 'Eritrea',\n",
       " 'to',\n",
       " 'Ethiopia',\n",
       " 'and',\n",
       " 'downgrading',\n",
       " 'the',\n",
       " 'operation',\n",
       " 'to',\n",
       " 'either',\n",
       " 'an',\n",
       " 'observer',\n",
       " 'or',\n",
       " 'liaison',\n",
       " 'effort',\n",
       " '.']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sent[2990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "x=1\n",
    "print(len(train_labels[x]))\n",
    "print(len(train_sent[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner.load_embed_vects(embedding_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_sent, nn_train_pos, nn_train_labels = ner.prep_train_for_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(nn_train_sent[x]))\n",
    "print(len(nn_train_labels[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner.batch_starting_point = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1, labels_1 = next(ner.nn_train_batch_generator(nn_train_sent, nn_train_labels, nn_train_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_1.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_hidden_dim, number_of_tags):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        #maps each token to an embedding_dim vector\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #the LSTM takens embedded sentence\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True)\n",
    "\n",
    "        #fc layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, number_of_tags)\n",
    "    \n",
    "    def forward(self, s):\n",
    "        #apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim\n",
    "\n",
    "        #run the LSTM along the sentences of length batch_max_len\n",
    "        s, _ = self.lstm(s)     # dim: batch_size x batch_max_len x lstm_hidden_dim                \n",
    "\n",
    "        #reshape the Variable so that each row contains one token\n",
    "        s = s.reshape(-1, s.shape[2])  # dim: batch_size*batch_max_len x lstm_hidden_dim\n",
    "\n",
    "        #apply the fully connected layer and obtain the output for each token\n",
    "        s = self.fc(s)          # dim: batch_size*batch_max_len x num_tags\n",
    "\n",
    "        return F.log_softmax(s, dim=1)   # dim: batch_size*b\n",
    "    \n",
    "    def loss_fn(outputs, labels):\n",
    "        #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "        labels = labels.view(-1)  \n",
    "\n",
    "        #mask out 'PAD' tokens\n",
    "        mask = (labels >= 1).float()\n",
    "        \n",
    "        num_tokens = int(torch.sum(mask))\n",
    "\n",
    "        #pick the values corresponding to labels and multiply by mask\n",
    "        \n",
    "        #NOT SURE I UNDERSTAND THIS NEXT LINE, I\"ll need to break it down\n",
    "        outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
    "\n",
    "        #cross entropy loss for all non 'PAD' tokens\n",
    "        return -torch.sum(outputs)/num_tokens\n",
    "    \n",
    "    def myCrossEntropyLoss(outputs, labels):\n",
    "        batch_size = outputs.size()[0]            # batch_size\n",
    "        outputs = F.log_softmax(outputs, dim=1)   # compute the log of softmax values\n",
    "        outputs = outputs[range(batch_size), labels] # pick the values corresponding to the labels\n",
    "        return -torch.sum(outputs)/num_examples\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK_NE': 1,\n",
       " 'PAD': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-tim': 2,\n",
       " 'I-gpe': 3,\n",
       " 'B-gpe': 4,\n",
       " 'I-eve': 5,\n",
       " 'B-nat': 6,\n",
       " 'B-per': 7,\n",
       " 'B-eve': 8,\n",
       " 'I-tim': 9,\n",
       " 'I-per': 10,\n",
       " 'I-org': 11,\n",
       " 'I-nat': 12,\n",
       " 'I-geo': 13,\n",
       " 'B-org': 14,\n",
       " 'O': 15,\n",
       " 'B-art': 16,\n",
       " 'I-art': 17}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.ne_tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (torch.sum(torch.tensor([1,1,1,1])))\n",
    "print(a)\n",
    "b = a.data.item()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(ner.vocab_size, 50, 20, len(ner.ne_tag_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Current loss: ---------  tensor(2.7848, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.155\n",
      "100 Current loss: ---------  tensor(2.4572, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.4165\n",
      "200 Current loss: ---------  tensor(1.7847, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.4845\n",
      "300 Current loss: ---------  tensor(1.1606, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.496\n",
      "400 Current loss: ---------  tensor(0.9553, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.5\n",
      "500 Current loss: ---------  tensor(0.8677, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.5035\n",
      "600 Current loss: ---------  tensor(0.8134, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.5035\n",
      "700 Current loss: ---------  tensor(0.7723, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.505\n",
      "800 Current loss: ---------  tensor(0.7375, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.5045\n",
      "900 Current loss: ---------  tensor(0.7064, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.5045\n",
      "1000 Current loss: ---------  tensor(0.6775, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.5045\n",
      "1100 Current loss: ---------  tensor(0.6498, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.5045\n",
      "1200 Current loss: ---------  tensor(0.6226, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.505\n",
      "1300 Current loss: ---------  tensor(0.5955, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.505\n",
      "1400 Current loss: ---------  tensor(0.5690, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.505\n",
      "1500 Current loss: ---------  tensor(0.5434, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.505\n",
      "1600 Current loss: ---------  tensor(0.5183, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.5065\n",
      "1700 Current loss: ---------  tensor(0.4936, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.507\n",
      "1800 Current loss: ---------  tensor(0.4695, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.509\n",
      "1900 Current loss: ---------  tensor(0.4461, grad_fn=<DivBackward0>)\n",
      "    Accuracy is ----------  0.511\n"
     ]
    }
   ],
   "source": [
    "def accuracy(out, labels):\n",
    "    labels = labels.ravel()\n",
    "    \n",
    "    mask = (labels >= 0)\n",
    "        \n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    \n",
    "    \n",
    "    return np.sum(outputs==labels)/float(labels.size)\n",
    "\n",
    "ner.batch_starting_point = 0\n",
    "train_iterator = ner.nn_train_batch_generator(nn_train_sent, nn_train_labels)\n",
    "\n",
    "for i in range(2000):\n",
    "\n",
    "    batch_sentences, batch_ne_labels = next(train_iterator)\n",
    "    \n",
    "#     batch_sentences = batch_sentences.contiguous()\n",
    "#     batch_ne_labels = batch_ne_labels.contiguous()\n",
    "    \n",
    "    output_batch = model(batch_sentences)\n",
    "    \n",
    "    loss = Net.loss_fn(output_batch, batch_ne_labels)\n",
    "    if i%100 == 0:\n",
    "        print(i, \"Current loss: --------- \", loss)\n",
    "        \n",
    "     \n",
    "    \n",
    "    # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "    output_batch = output_batch.data.cpu().numpy()\n",
    "    batch_ne_labels = batch_ne_labels.data.cpu().numpy()\n",
    "    \n",
    "#     loss = Net.myCrossEntropyLoss(output_batch, batch_ne_labels)\n",
    "    optimizer.zero_grad()  # clear previous gradients\n",
    "    loss.backward()        # compute gradients of all variables wrt loss\n",
    "    optimizer.step()       # perform updates using calculated gradients\n",
    "    if i%100 == 0:\n",
    "        print(\"    Accuracy is ---------- \", accuracy(output_batch, batch_ne_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
